# -*- coding: utf-8 -*-
"""Heart Disease Detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RwAOa6cVa6fJAdLVoFFfQoWYO7jktdQM

##**Heart Disease Detection**##

Deteksi Penyakit Jantung dibuat menggunakan *Machine Learning* dengan Algoritma *Artificial Neural Network (ANN) Backpropagation Methods*
"""

# Commented out IPython magic to ensure Python compatibility.
# Import Library
import warnings; warnings.simplefilter ('ignore')
import pandas as pd
import seaborn as sns
import numpy as np
# %matplotlib inline
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn import tree
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score, accuracy_score
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.pipeline import make_pipeline
from sklearn.impute import SimpleImputer
print(tf.__version__)

# Upload File
from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

# Visualize the Data
df = pd.read_csv("heart.csv")
df.head(10)

N, P = df.shape # Data Size
print('Baris = ', N, '\nKolom (jumlah variabel) = ', P)
print("Tipe Variabe df = ", type(df))
df

# Look the data info
df.info()

# List Column
df.columns

# Summary Data
df.describe()

# Looking at the Missing Values
df.isnull().sum()

# Look at unique data
df = df.select_dtypes(include=['float64', 'int64'])

# Print unique values for each numeric column
for col in df.columns:
    unique_values = set(df[col].unique())
    print(f"{col} : {unique_values}")

# Drop the 'age' column from the DataFrame
df_dropped= df.drop(columns=['age'])

# Check the DataFrame to ensure 'age' column is dropped
df_dropped

from sklearn.preprocessing import StandardScaler, MinMaxScaler

numeric_columns = ['trestbps','chol','thalach','oldpeak','ca']

scaler = StandardScaler()
numeric_df = df_dropped[numeric_columns]

numeric_scaled_df = pd.DataFrame(scaler.fit_transform(numeric_df), columns = scaler.get_feature_names_out())
numeric_scaled_df.head()

from sklearn.preprocessing import OneHotEncoder

nominal_columns = ['cp','restecg','slope','thal']
ohc = OneHotEncoder(sparse_output=False, drop=None)

dummies_df = pd.DataFrame(ohc.fit_transform(df[nominal_columns]), columns = ohc.get_feature_names_out())
dummies_df

df_clean_data = pd.concat([numeric_scaled_df, dummies_df, df[['sex', 'fbs','exang','target']]], axis=1)
df_clean_data

"""Lihat persebaran datanya melalui plot histogram"""

from collections import Counter

# sns.countplot for visualization
sns.countplot(data=df_clean_data, x='target')
plt.show()

# Count occurrences of each value in the 'target' column
D = Counter(df_clean_data['target'])
print(D)

# Calculate and print percentages
total = len(df_clean_data['target'])
yes_count = D[1] if 1 in D else 0  # Assuming 'YES' is represented by 1
no_count = D[0] if 0 in D else 0   # Assuming 'NO' is represented by 0

print("YES =", yes_count * 100 / total, '% NO =', no_count * 100 / total, '%')

# Assume df is your original DataFrame
df_encode_data = df_clean_data.copy()

# Ensure 'sex' column is present and convert to 'object' type if needed
if 'sex' in df_encode_data.columns:
    df_encode_data['sex'] = df_encode_data['sex'].astype('object')
else:
    print("The 'sex' column does not exist in the DataFrame")

# One-Hot Encoding for 'sex' column
transformation_gender = pd.get_dummies(df_encode_data['sex'], prefix='sex')
encode_data = pd.concat([df_encode_data, transformation_gender], axis=1)

# Debug: Print column names to verify encoding
print("Columns after one-hot encoding:", encode_data.columns)

# Remove 'sex' column after one-hot encoding
try:
    encode_data.drop(columns=['sex'], axis=1, inplace=True)
except KeyError as err_:
    print(err_)

# Verify the columns to move exist
cols_to_move = [col for col in encode_data.columns if col.startswith('sex_')]
print("Columns to move:", cols_to_move)

# Move columns 'sex_0' and 'sex_1' before 'target'
remaining_cols = [col for col in encode_data.columns if col not in cols_to_move]
df_encode_data = encode_data[cols_to_move + remaining_cols]

# Convert boolean columns to float64
df_encode_data['sex_0'] = df_encode_data['sex_0'].astype('float64')
df_encode_data['sex_1'] = df_encode_data['sex_1'].astype('float64')

# See the latest data after using the 'One-Hot Encoding' technique
J, K = df_encode_data.shape  # Data Size
print('Baris terbaru = ', J, '\nKolom (jumlah variabel) terbaru = ', K)
print("Tipe Variabel Data Encoding = ", type(df_encode_data))
df_encode_data

"""Lihat data setelah dilakukan encoding"""

# Looking the data info after encode
df_encode_data.info()

"""**Artificial Neural Network (ANN)**

Dalam membuat dan melatih model ANN, ada library yang disediakan oleh Google langsung bernama Tensorflow.

Sebelum melatih model, alangkah baiknya diubah dulu jenis data 'int64' menjadi 'float64'. Alasannya karena ketika melatih model, model akan dikonversikan terlebih dahulu menjadi 'Tensorflow Dataset'.

Tensorflow sendiri hanya mendukung variabel 'float' bukan 'int' ketika ingin dikonversikan ke dalam numpy.
"""

# Convert Pandas Dataframe to Tensorflow Dataset
df_tf_convert = df_encode_data.copy()
print("Kolom yang ada di dalam :\n",df_tf_convert.columns)

# Remove whitespaces from column names
df_tf_convert.columns = df_tf_convert.columns.str.strip()

# Update columns_to_convert_float_corrected
columns_to_convert_float_corrected = ['sex_0', 'sex_1', 'trestbps', 'chol', 'thalach',
                                      'oldpeak','ca','cp_0','cp_1','cp_2','cp_3', 'restecg_0',
                                      'restecg_1', 'restecg_2', 'slope_0', 'slope_1', 'slope_2',
                                      'thal_0', 'thal_1', 'thal_2', 'thal_3',
                                      'fbs', 'exang', 'target']

# Convert specified columns to float64
df_tf_convert[columns_to_convert_float_corrected] = df_tf_convert[columns_to_convert_float_corrected].astype('float64')

# Look the data after updating
df_tf_convert

# Looking the data update
df_tf_convert.info()

# Set x and y
x = df_tf_convert.drop('target', axis=1)
y = df_tf_convert['target']

# Check initial class distribution
initial_distribution = y.value_counts()
print("Class distribution:\n", initial_distribution)

from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline

# Define the desired number of samples for each class
desired_class_count = 530

# Define the over-sampling and under-sampling steps
over = SMOTE(sampling_strategy={0: desired_class_count, 1: desired_class_count}, random_state=42)
under = RandomUnderSampler(sampling_strategy={0: desired_class_count, 1: desired_class_count}, random_state=42)
steps = [('over', over), ('under', under)]
pipeline = Pipeline(steps=steps)

# Apply the resampling
x_resampled, y_resampled = pipeline.fit_resample(x, y)

# Print the final class distribution
final_distribution = pd.Series(y_resampled).value_counts()
print("Class distribution after resampling:\n", final_distribution)
print(f"Jumlah data dalam x_resampled sekarang: {len(x_resampled)} dan jumlah data y_resampled sekarang: {len(y_resampled)}")

"""Lakukan splitting dataset"""

# Splitting into train-test split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Convert Dataframe to Tensorflow Dataset
train_dataset = tf.data.Dataset.from_tensor_slices((x_train.values, y_train.values))
test_dataset = tf.data.Dataset.from_tensor_slices((x_test.values, y_test.values))

print("Ukuran Input (x) Training  =", x_train.shape, "\nUkuran Output (y) Training =", y_train.shape)
print("Ukuran Input (x) Testing  =", x_test.shape, "\nUkuran Output (y) Testing =", y_test.shape)

model = tf.keras.Sequential([
    tf.keras.layers.Dense(1038, input_shape=(len(x_train.columns),), activation='leaky_relu', name="dense_1"),
    tf.keras.layers.Dense(512, activation='leaky_relu', name="dense_2"),
    tf.keras.layers.Dense(132, activation='leaky_relu', name="dense_3"),
    tf.keras.layers.Dense(32, activation='leaky_relu', name="dense_4"),
    tf.keras.layers.Dense(1, activation='sigmoid', name="dense_5"),
])

model.summary()

# Define w and b as trainable variables
W1, b1 = model.get_layer("dense_1").get_weights()
W2, b2 = model.get_layer("dense_2").get_weights()
W3, b3 = model.get_layer("dense_3").get_weights()
W4, b4 = model.get_layer("dense_4").get_weights()
W5, b5 = model.get_layer("dense_5").get_weights()
print(f"W1{W1.shape}:\n", W1, f"\nb1{b1.shape}:", b1)
print(f"W2{W2.shape}:\n", W2, f"\nb2{b2.shape}:", b2)
print(f"W3{W3.shape}:\n", W3, f"\nb1{b3.shape}:", b3)
print(f"W4{W4.shape}:\n", W4, f"\nb2{b4.shape}:", b4)
print(f"W5{W5.shape}:\n", W5, f"\nb2{b5.shape}:", b5)

# Define the optimizer, loss function, and metrics
optimizer_function = tf.keras.optimizers.Adam()
loss_function = tf.keras.losses.BinaryCrossentropy()

# Compile model
model.compile(optimizer=optimizer_function,
              loss=loss_function,
              metrics=['accuracy'])

# Training loop with tf.GradientTape
@tf.function
def train_step(inputs, targets):
    with tf.GradientTape() as tape:
        predictions = model(inputs, training=True)
        loss = loss_function(targets, predictions)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer_function.apply_gradients(zip(gradients, model.trainable_variables))
    return loss, predictions

# Training the model
epochs = 20
train_losses = []
train_accuracies = []
val_losses = []
val_accuracies = []

for epoch in range(epochs):
    epoch_loss_avg = tf.keras.metrics.Mean()
    epoch_accuracy = tf.keras.metrics.BinaryAccuracy()

    # Training loop with batching
    for x_batch, y_batch in train_dataset.batch(32):
        loss_value, predictions = train_step(x_batch, y_batch)
        epoch_loss_avg.update_state(loss_value)
        epoch_accuracy.update_state(y_batch, predictions)

    # Calculate validation loss and accuracy
    val_loss, val_accuracy = model.evaluate(x_test, y_test, verbose=2)

    # Append to lists for plotting
    train_losses.append(epoch_loss_avg.result())
    train_accuracies.append(epoch_accuracy.result())
    val_losses.append(val_loss)
    val_accuracies.append(val_accuracy)

    # Display metrics at the end of each epoch
    print(f"Epoch {epoch + 1}: Loss: {epoch_loss_avg.result()}, Accuracy: {epoch_accuracy.result()}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}")

# Plot Training and Validation Accuracy
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)  # 1 row, 2 columns, plot 1
plt.plot(train_accuracies, label='Training Accuracy')
plt.plot(val_accuracies, label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()

# Plot Training and Validation Loss
plt.subplot(1, 2, 2)  # 1 row, 2 columns, plot 2
plt.plot(train_losses, label='Training Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()

plt.tight_layout()  # Adjust layout to prevent overlapping
plt.show()

# Predict testing set labels
y_test_pred = model.predict(x_test)
y_test_pred_binary = (y_test_pred > 0.5).astype(int)

# Convert predictions to binary labels for testing data
y_test_binary = (y_test > 0.5).astype(int)

# Compute confusion matrix for testing data
test_conf_matrix = confusion_matrix(y_test_binary, y_test_pred_binary)

# Compute classification report for testing data
test_class_report = classification_report(y_test_binary, y_test_pred_binary)

# Plot confusion matrix for testing data
plt.figure(figsize=(8, 6))
sns.heatmap(test_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix (Testing Data)')
plt.show()

print("Testing Data Confusion Matrix:")
print(test_conf_matrix)
print("\nTesting Data Classification Report:")
print(test_class_report)